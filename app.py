# -*- coding: utf-8 -*-
"""Smart Premium Insurance

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-NNy7BA7RYgFCgKdv0cW9kQS2iM0mylc
"""

pip install pandas numpy matplotlib seaborn xgboost streamlit mlflow

import pandas as pd
train = pd.read_csv('train.csv')
train.drop(columns = ["Policy Start Date"], inplace = True)
train.drop(columns = ["id"], inplace = True)
train.head()

import pandas as pd

train['Age group'] = pd.cut(
    train['Age'],
    bins=[0, 18, 30, 45, 60],
    labels=["<18", "18-30", "30-45", "45-60"],
    right=False  # Optional: to control whether upper bound is inclusive
)
train['Income_Bracket'] = pd.cut(
    train['Annual Income'],
    bins=[0, 30000, 60000, 100000, 130000, 160000],
    labels=['Low', 'Lower-Mid', 'Mid', 'Upper-Mid', 'High']
)
train['Health Level'] = pd.cut(
    train['Health Score'],
    bins=[0, 15, 30, 45, 60],
    labels=['Poor', 'Fair', 'Good', 'Excellent']
)
train['credit category'] = pd.cut(
    train['Credit Score'],
    bins=[300, 450, 600, 750, 850, 900],
    labels=['Poor', 'Fair', 'Good', 'Very Good', 'Excellent']
)
train['insurance item'] = pd.cut(
    train['Insurance Duration'],
    bins=[0,1,2,3,4],
    labels=["<1yr","1-2yrs","2-3yrs","3-4yrs"]
    )
train['Vehicle Type'] = pd.cut(
    train['Vehicle Age'],
    bins=[0,2,5,10,20],
    labels=["new","moderate","old","very old"]
    )


train.head()

# 1. Income per dependent
train['Income_per_Dependent'] = train['Annual Income'] / (train['Number of Dependents'] + 1)

# 2. Income per age (to normalize high earners by age)
train['Income_per_Age'] = train['Annual Income'] / (train['Age'] + 1)

# 3. Is young high earner (binary flag)
train['Is_Young_High_Earner'] = ((train['Age'] < 30) & (train['Annual Income'] > 75000)).astype(int)

# 4. Credit-to-Age Ratio
train['Credit_per_Age'] = train['Credit Score'] / (train['Age'] + 1)

train.shape

train.dtypes

train.describe()

import seaborn as sns
import matplotlib.pyplot as plt
sns.barplot(x="Gender", y="Premium Amount", data=train)
plt.show()

sns.scatterplot(x='Annual Income', y='Premium Amount', data= train)
plt.show()

sns.barplot(x="Marital Status", y="Premium Amount", data=train)
plt.show()

sns.scatterplot(x='Age', y='Premium Amount', data= train)
plt.show()

sns.scatterplot(x="Number of Dependents", y="Premium Amount", data= train)
plt.show()

sns.barplot(x="Policy Type", y="Premium Amount", data= train)
plt.show()

sns.scatterplot(x='Previous Claims', y='Premium Amount', data=train)
plt.show()

train["credit score status"] = pd.qcut(train["Credit Score"], q = 4, labels=["low", "medium", "high", "very high"])
train.groupby('credit score status')['Premium Amount'].mean().plot(kind='line', title='Average Premium by Credit Score Status')
plt.ylabel('Avg Premium Amount')
plt.show()

import matplotlib.pyplot as plt
train["Premium Amount"].hist(bins=10, color='skyblue')
plt.xlabel('Premium Amount')
plt.ylabel('Frequency')
plt.title('Distribution of Premium Amount')
plt.show()

train['Health Score'].hist(bins=10, color='purple')
plt.xlabel('Health Score')
plt.ylabel('Frequency')
plt.title('Distribution of Health Score')
plt.show()

num_cols = train.select_dtypes(include=['int64', 'float64']).columns
train[num_cols] = train[num_cols].fillna(train[num_cols].median())

numerical_cols = train.select_dtypes(include=['int64', 'float64']).columns
print(numerical_cols)

# Define categorical columns before one-hot encoding
categorical_cols = train.select_dtypes(include=['object', 'category']).columns
print(categorical_cols)

cat_cols = train.select_dtypes(include=['object', 'category']).columns
for col in cat_cols:
    train[col] = train[col].fillna(train[col].mode().iloc[0])

train = pd.get_dummies(train, columns=cat_cols)

train.dtypes

x = train.drop(columns = ["Premium Amount"], axis = 1)
y = train["Premium Amount"]

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Check outliers before removal
for col in numerical_cols:
    Q1 = train[col].quantile(0.25)
    Q3 = train[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = train[(train[col] < lower_bound) | (train[col] > upper_bound)]
    print(f"{col}: {len(outliers)} outliers")

# Remove outliers for original features
for col in numerical_cols:
    Q1 = train[col].quantile(0.25)
    Q3 = train[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    train = train[(train[col] >= lower_bound) & (train[col] <= upper_bound)]

# Feature engineering
train['Income_per_Dependent'] = train['Annual Income'] / (train['Number of Dependents'] + 1)
train['Income_per_Age'] = train['Annual Income'] / (train['Age'] + 1)
train['Is_Young_High_Earner'] = ((train['Age'] < 30) & (train['Annual Income'] > 75000)).astype(int)
train['Credit_per_Age'] = train['Credit Score'] / (train['Age'] + 1)

# Remove outliers from new features
new_features = ['Income_per_Dependent', 'Income_per_Age', 'Credit_per_Age']
for col in new_features:
    Q1 = train[col].quantile(0.25)
    Q3 = train[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    train = train[(train[col] >= lower_bound) & (train[col] <= upper_bound)]



# Recreate DataFrames with column names if x_test is currently an array
x_train = pd.DataFrame(x_train, columns=x.columns)
x_test = pd.DataFrame(x_test, columns=x.columns)

# 1. Income per dependent (avoids division by zero by adding 1)
x_train['Income_per_Dependent'] = x_train['Annual Income'] / (x_train['Number of Dependents'] + 1)
x_test['Income_per_Dependent'] = x_test['Annual Income'] / (x_test['Number of Dependents'] + 1)

# 2. Income per age
x_train['Income_per_Age'] = x_train['Annual Income'] / (x_train['Age'] + 1)
x_test['Income_per_Age'] = x_test['Annual Income'] / (x_test['Age'] + 1)

# 3. Binary feature: young high earner
x_train['Is_Young_High_Earner'] = ((x_train['Age'] < 30) & (x_train['Annual Income'] > 75000)).astype(int)
x_test['Is_Young_High_Earner'] = ((x_test['Age'] < 30) & (x_test['Annual Income'] > 75000)).astype(int)

# 4. Credit score per age
x_train['Credit_per_Age'] = x_train['Credit Score'] / (x_train['Age'] + 1)
x_test['Credit_per_Age'] = x_test['Credit Score'] / (x_test['Age'] + 1)

# Drop weak features (based on importance plot)
low_impact_cols = [
    'Marital Status_Divorced', 'Marital Status_Married', 'Marital Status_Single',
    "Education Level_Bachelor's", 'Education Level_High School', "Education Level_Master's", 'Education Level_PhD',
    'Occupation_Employed', 'Occupation_Self-Employed', 'Occupation_Unemployed',
    'Location_Rural', 'Location_Suburban', 'Location_Urban',
    'Policy Type_Basic', 'Policy Type_Comprehensive', 'Policy Type_Premium',
    'Customer Feedback_Average', 'Customer Feedback_Good', 'Customer Feedback_Poor',
    'Smoking Status_No', 'Smoking Status_Yes',
    'Exercise Frequency_Daily', 'Exercise Frequency_Monthly', 'Exercise Frequency_Rarely', 'Exercise Frequency_Weekly',
    'Property Type_Apartment', 'Property Type_Condo', 'Property Type_House',
    # You can skip dropping age groups, bracketed features if unsure.
]

# Convert back to DataFrame before dropping columns
x_train = pd.DataFrame(x_train, columns=x.columns)
x_test = pd.DataFrame(x_test, columns=x.columns)

x_train = x_train.drop(columns=low_impact_cols, errors='ignore')
x_test = x_test.drop(columns=low_impact_cols, errors='ignore')

from sklearn.linear_model import LinearRegression
model_linear = LinearRegression()
model_linear.fit(x_train, y_train)

from sklearn.tree import DecisionTreeRegressor
model_tree = DecisionTreeRegressor(random_state=42)
model_tree.fit(x_train, y_train)

from sklearn.ensemble import RandomForestRegressor
model_rf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
model_rf.fit(x_train, y_train)

from xgboost import XGBRegressor
import re

def clean_col_names(df):
    cols = df.columns
    new_cols = []
    for col in cols:
        new_col = re.sub(r'[^A-Za-z0-9_]+', '', col)  # Remove invalid characters
        new_cols.append(new_col)
    df.columns = new_cols
    return df

# Clean column names before fitting the model
x_train_cleaned = clean_col_names(pd.DataFrame(x_train, columns=x.columns))
x_test_cleaned = clean_col_names(pd.DataFrame(x_test, columns=x.columns))

model_xgb = XGBRegressor(random_state=42)
model_xgb.fit(x_train_cleaned, y_train)

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np

def evaluate_model(model, x_test, y_test, name = "Model"):
    y_pred = model.predict(x_test)
    rsme = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"{name} Performance: ")
    print(f"RSME {rsme: 2f}")
    print(f"MAE {mae: 2f}")
    print(f"R2 {r2: 2f}")

    return {"Model": name, "RSME": rsme, "MAE": mae, "R2": r2}

results = []
results.append(evaluate_model(model_linear, x_test, y_test, name = "Linear Regression"))
results.append(evaluate_model(model_tree, x_test, y_test, name = "Decision Tree"))
results.append(evaluate_model(model_rf, x_test, y_test, name = "Random Forest"))
results.append(evaluate_model(model_xgb, x_test_cleaned, y_test, name = "XGBoost"))

import pandas as pd
import matplotlib.pyplot as plt

# After fitting your XGBoost model
importances = model_xgb.feature_importances_
feature_names = x_train_cleaned.columns # Use cleaned column names

# Sort and display top 20
feat_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10,6))
plt.barh(feat_imp_df['Feature'][:20], feat_imp_df['Importance'][:20])
plt.gca().invert_yaxis()
plt.title("Top 20 Feature Importances (XGBoost)")
plt.show()

import numpy as np
import pandas as pd
import re

# Define the clean_col_names function if it's not already defined in this cell
def clean_col_names(df):
    cols = df.columns
    new_cols = []
    for col in cols:
        new_col = re.sub(r'[^A-Za-z0-9_]+', '', col)  # Remove invalid characters
        new_cols.append(new_col)
    df.columns = new_cols
    return df

# Before model training
y_train_log = np.log1p(y_train)
y_test_log = np.log1p(y_test)

# Clean column names before fitting the model
x_train_cleaned = clean_col_names(pd.DataFrame(x_train, columns=x.columns))
x_test_cleaned = clean_col_names(pd.DataFrame(x_test, columns=x.columns))


# Use one of the trained models, e.g., model_xgb
model_xgb.fit(x_train_cleaned, y_train_log)
y_pred_log = model_xgb.predict(x_test_cleaned)

# Inverse the log prediction
y_pred = np.expm1(y_pred_log)

# Then calculate metrics on original scale

from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBRegressor
import scipy.stats as stats
import pandas as pd
import re

def clean_col_names(df):
    cols = df.columns
    new_cols = []
    for col in cols:
        new_col = re.sub(r'[^A-Za-z0-9_]+', '', col)
        new_cols.append(new_col)
    df.columns = new_cols
    return df


xgb = XGBRegressor()

param_dist = {
    'max_depth': [3, 5, 7],
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.7, 1.0]
}

random_search = RandomizedSearchCV(
    xgb,
    param_distributions=param_dist,
    n_iter=10,  # Only try 10 random combinations
    scoring='neg_root_mean_squared_error',
    cv=3,
    verbose=1,
    n_jobs=-1,  # Use all available CPU cores
    random_state=42
)

x_train_cleaned = clean_col_names(pd.DataFrame(x_train, columns=x.columns))

random_search.fit(x_train_cleaned, y_train)
best_model = random_search.best_estimator_

from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
import pandas as pd
import re

# Define the clean_col_names function if it's not already defined in this cell
def clean_col_names(df):
    cols = df.columns
    new_cols = []
    for col in cols:
        new_col = re.sub(r'[^A-Za-z0-9_]+', '', col)  # Remove invalid characters
        new_cols.append(new_col)
    df.columns = new_cols
    return df

# Ensure x_train and x_test are DataFrames with original column names before cleaning
x_train_orig = pd.DataFrame(x_train, columns=x.columns)
x_test_orig = pd.DataFrame(x_test, columns=x.columns)

# Apply column cleaning to both training and testing data
x_train_cleaned = clean_col_names(x_train_orig.copy())
x_test_cleaned = clean_col_names(x_test_orig.copy())


# Retrain models on the cleaned data
model_rf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
model_rf.fit(x_train_cleaned, y_train)

model_xgb = XGBRegressor(random_state=42)
model_xgb.fit(x_train_cleaned, y_train)

# Make predictions using the models trained on cleaned data
final_pred = (model_rf.predict(x_test_cleaned) + model_xgb.predict(x_test_cleaned)) / 2

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Evaluate final predictions
rmse = np.sqrt(mean_squared_error(y_test, final_pred))
mae = mean_absolute_error(y_test, final_pred)
r2 = r2_score(y_test, final_pred)

print("Final Ensemble Model Performance:")
print("RMSE:", rmse)
print("MAE:", mae)
print("R2:", r2)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from xgboost import XGBRegressor
import pandas as pd

# Assuming you have:
# - x_train, x_test, y_train, y_test

# Convert x_train and x_test back to DataFrames to work with column names
# Use the columns from the original 'x' DataFrame as a reference for column names
x_train_df = pd.DataFrame(x_train, columns=x.columns)
x_test_df = pd.DataFrame(x_test, columns=x.columns)

# Identify numerical and categorical columns from the DataFrames *after* feature selection
numerical_cols_post_selection = x_train_df.select_dtypes(include=['float64', 'int64']).columns
encoded_categorical_cols_post_selection = x_train_df.select_dtypes(include=['bool']).columns # Assuming one-hot encoded are boolean


# 1. Define Preprocessing for Numerical and Categorical
# Numerical columns are already scaled, so we just need imputation
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median'))
    # Scaling is already done, so no StandardScaler here
])

# Categorical columns are already one-hot encoded, so we just need imputation and passthrough
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', 'passthrough')  # Already one-hot encoded
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_cols_post_selection),
        ('cat', categorical_transformer, encoded_categorical_cols_post_selection)
    ],
    remainder='passthrough' # Keep any other columns (like engineered features that are not float/int/bool)
)

# 2. Define Pipeline
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', XGBRegressor(n_estimators=100, max_depth=5, random_state=42))
])


# 3. Train Model
model.fit(x_train_df, y_train)

# 4. Evaluate
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

y_pred = model.predict(x_test_df)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("RMSE:", rmse)
print("MAE:", mae)
print("R2:", r2)

# 1. Install MLflow and pyngrok if not installed
!pip install mlflow pyngrok

# 2. Import libraries
import subprocess
import threading
import time
from pyngrok import ngrok

# 3. Set your ngrok auth token here (🔴 Replace with your actual token)
ngrok.set_auth_token("30KwR5ga0Vn9OtYNtI04zeZnnVL_6MiSjKv2387sQbijjpHSS")  # 👈 paste token here

# 4. Start MLflow UI in background
mlflow_port = 5000  # Use 5000 or any other available port

def run_mlflow():
    cmd = ["mlflow", "ui", "--port", str(mlflow_port)]
    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

# 5. Start MLflow in a new thread
mlflow_thread = threading.Thread(target=run_mlflow)
mlflow_thread.daemon = True
mlflow_thread.start()

# 6. Wait for MLflow to start
time.sleep(5)

# 7. Create public URL using ngrok
try:
    public_url = ngrok.connect(mlflow_port)
    print(f"✅ MLflow UI is running at: {public_url}")
except Exception as e:
    print(f"❌ Could not connect to ngrok. Error: {e}")

import mlflow
import mlflow.sklearn
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from xgboost import XGBRegressor
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Re-split the data
x = train.drop(columns=["Premium Amount"])
y = train["Premium Amount"]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Column types
numerical_cols = x_train.select_dtypes(include=np.number).columns.tolist()
categorical_cols = x_train.select_dtypes(include=['object', 'category']).columns.tolist()

# Transformers
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Column transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ],
    remainder='passthrough'
)

# Pipeline with model
model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', XGBRegressor(n_estimators=100, max_depth=5, random_state=42))
])

# ✅ Update this to your ngrok MLflow link:
mlflow.set_tracking_uri("http://localhost:5000")  # replace with actual URL

mlflow.set_experiment("SmartPremium")

# MLflow tracking
with mlflow.start_run():
    # Train
    model_pipeline.fit(x_train, y_train)
    y_pred = model_pipeline.predict(x_test)

    # Metrics
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    from mlflow.models.signature import infer_signature
    signature = infer_signature(x_train, model_pipeline.predict(x_train))

    # ✅ Log model with signature
    mlflow.sklearn.log_model(
        sk_model=model_pipeline,
        artifact_path="premium_prediction_model",
        signature=signature
    )

    mlflow.log_metric("rmse", rmse)
    mlflow.log_metric("mae", mae)
    mlflow.log_metric("r2", r2)

    # Parameters
    mlflow.log_param("model_type", "XGBoost")
    mlflow.log_param("n_estimators", 100)
    mlflow.log_param("max_depth", 5)

    print("✅ MLflow run completed.")
    print(f"RMSE: {rmse:.2f}")
    print(f"MAE: {mae:.2f}")
    print(f"R2: {r2:.2f}")

# 1. Import and preprocess data
# 2. Define preprocessor
# 3. Train initial model (optional)
# 4. Setup MLflow
import numpy as np

mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("SmartPremium")

# ✅ 5. Now paste and run this tuning loop at the end:
from xgboost import XGBRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score

for n in [100,200,300,500]:
    with mlflow.start_run(run_name=f"XGB_{n}_trees"):
        model = XGBRegressor( n_estimators=n,
    max_depth=3 or 4,
    learning_rate=0.1 or 0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42)
        pipeline = Pipeline([
            ("preprocessor", preprocessor),
            ("regressor", model)
        ])
        pipeline.fit(x_train, y_train)
        preds = pipeline.predict(x_test)


        rmse = np.sqrt(mean_squared_error(y_test, preds))
        mae = mean_absolute_error(y_test, preds)

        r2 = r2_score(y_test, preds)

        mlflow.log_param("n_estimators", n)
        mlflow.log_metric("rmse", rmse)
        mlflow.log_metric("mae", mae)
        mlflow.log_metric("r2", r2)
        mlflow.sklearn.log_model(pipeline, "xgb_model")

import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from xgboost import XGBRegressor
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings("ignore")

# === 1. Load and Prepare Data ===
data = pd.read_csv("train.csv")  # Change to your CSV file

# Impute missing values in the target variable
imputer = SimpleImputer(strategy='median')
data['Premium Amount'] = imputer.fit_transform(data[['Premium Amount']])

# Target
y = data['Premium Amount']
X = data.drop(columns='Premium Amount')

# Train-test split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# === 2. Preprocessing ===
numeric_features = x_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = x_train.select_dtypes(include=['object']).columns.tolist()

preprocessor = ColumnTransformer([
    ("num", Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numeric_features),
    ("cat", Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))]), categorical_features)
])


# === 3. MLflow setup ===
mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("SmartPremium")

# === 4. Train & Track with MLflow ===
with mlflow.start_run(run_name="XGB_100_Clean"):
    model = XGBRegressor(n_estimators=100, max_depth=5, random_state=42)
    pipeline = Pipeline([
        ("preprocessor", preprocessor),
        ("regressor", model)
    ])

    pipeline.fit(x_train, y_train)
    preds = pipeline.predict(x_test)

    # Metrics
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    mae = mean_absolute_error(y_test, preds)
    r2 = r2_score(y_test, preds)

    # Log
    mlflow.log_param("model", "XGBRegressor")
    mlflow.log_param("n_estimators", 100)
    mlflow.log_param("max_depth", 5)
    mlflow.log_metric("rmse", rmse)
    mlflow.log_metric("mae", mae)
    mlflow.log_metric("r2", r2)

    # Log input example and signature
    input_example = x_test.iloc[:1]
    mlflow.sklearn.log_model(pipeline, "model",
                             input_example=input_example)

    print("✅ MLflow run completed.")
    print(f"RMSE: {rmse:.2f}")
    print(f"MAE: {mae:.2f}")
    print(f"R2: {r2:.4f}")

"""The code above defines a pipeline that handles the preprocessing (imputation, scaling, one-hot encoding) and the XGBoost model training. It then trains this pipeline on the `x_train` data and logs the model and metrics to MLflow. Remember to run the cell that starts the MLflow UI (`RaYS-DWVU7Ac`) before running this cell to view the logged runs."""

import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.impute import SimpleImputer
from mlflow.models.signature import infer_signature
import warnings
warnings.filterwarnings("ignore")

# === 1. Load and Prepare Data ===
data = pd.read_csv("train.csv")

# Drop irrelevant columns if present
cols_to_drop = ['id', 'Patient ID', 'Policy ID', 'Policy Start Date'] # Added Policy Start Date back for consistency
data = data.drop(columns=[col for col in cols_to_drop if col in data.columns])

# Ensure 'smoker' column exists (using 'Smoking Status' from the dataset)
if 'smoker' not in data.columns and 'Smoking Status' in data.columns:
    data['smoker'] = data['Smoking Status'].apply(lambda x: 'yes' if x == 'Yes' else 'no')
elif 'smoker' not in data.columns: # If 'Smoking Status' is also missing, create a random 'smoker' column
    data['smoker'] = np.random.choice(['yes', 'no'], size=len(data))


# Impute missing values in the target variable before feature engineering
imputer = SimpleImputer(strategy='median')
data['Premium Amount'] = imputer.fit_transform(data[['Premium Amount']])

# Apply log transform to the target variable
data['Premium Amount'] = np.log1p(data['Premium Amount'])


# === 2. Feature Engineering ===
# Only create features if base columns exist
if "Age" in data.columns and "smoker" in data.columns:
    data["age_smoker"] = data["Age"] * (data["smoker"] == "yes").astype(int)

if "Number of Children" in data.columns and "Age" in data.columns:
    data["children_per_age"] = data["Number of Children"] / (data["Age"] + 1)


# === 3. Define Target and Features ===
y = data['Premium Amount'] # Now log-transformed and imputed
X = data.drop(columns='Premium Amount')

# Split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# === 4. Preprocessing Pipelines ===
numerical_features = x_train.select_dtypes(include=np.number).columns.tolist()
categorical_features = x_train.select_dtypes(include=['object', 'category']).columns.tolist() # Include 'category' as some created features might be categories

# Ensure no overlap between numerical and categorical lists after selecting dtypes
common_cols = list(set(numerical_features) & set(categorical_features))
if common_cols:
    print(f"Warning: Columns appear in both numerical and categorical lists: {common_cols}")
    # Decide how to handle - here we prioritize numerical if it's a number
    for col in common_cols:
        if col in categorical_features:
            categorical_features.remove(col)


numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')), # Added imputer here as well to catch any NaNs introduced by feature engineering
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')), # Added imputer
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='drop' # Explicitly drop other columns
)

# === 5. MLflow Setup ===
mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("SmartPremium")

# === 6. Define Models ===
models = {
    "XGB_100": XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42),
    "XGB_500_lr0.05": XGBRegressor(n_estimators=500, max_depth=6, learning_rate=0.05, random_state=42),
    "Linear": LinearRegression()
}

# === 7. Model Training & MLflow Logging ===
for name, model in models.items():
    with mlflow.start_run(run_name=name) as run:
        pipeline = Pipeline([
            ("preprocessor", preprocessor),
            ("regressor", model)
        ])
        pipeline.fit(x_train, y_train)
        preds = pipeline.predict(x_test)

        # Undo log transform for evaluation metrics
        y_test_orig = np.expm1(y_test)
        preds_orig = np.expm1(preds)

        rmse = np.sqrt(mean_squared_error(y_test_orig, preds_orig))
        mae = mean_absolute_error(y_test_orig, preds_orig)
        r2 = r2_score(y_test_orig, preds_orig)

        mlflow.log_param("model", name)
        if hasattr(model, 'n_estimators'):
            mlflow.log_param("n_estimators", model.n_estimators)
        if hasattr(model, 'max_depth'): # Log max_depth as well
            mlflow.log_param("max_depth", model.max_depth)


        mlflow.log_metric("rmse", rmse)
        mlflow.log_metric("mae", mae)
        mlflow.log_metric("r2", r2)

        # Log model with signature and example
        input_example = x_test.iloc[:1]
        signature = infer_signature(x_test, preds)
        mlflow.sklearn.log_model(pipeline, "model", input_example=input_example, signature=signature)

        print(f"✅ Run: {name} | RMSE={rmse:.2f} | MAE={mae:.2f} | R²={r2:.4f}")

        # Optional: register best model manually from UI or use below line with run.info.run_id
        # mlflow.register_model(f"runs:/{run.info.run_id}/model", "SmartPremiumBestModel")

print("\n✅ All model runs completed.")

# Install necessary libraries
!pip install -q streamlit pyngrok

# --- 1. Set your new ngrok auth token ---
from pyngrok import ngrok

# Replace with your new ngrok token (from https://dashboard.ngrok.com/tunnels/authtokens)
ngrok.set_auth_token("30KwR5ga0Vn9OtYNtI04zeZnnVL_6MiSjKv2387sQbijjpHSS")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import streamlit as st
# import pandas as pd
# import mlflow.pyfunc
# 
# # Sidebar Navigation
# st.sidebar.title("Navigation")
# page = st.sidebar.radio("Go to", ["🏠 Project Introduction", "📈 Premium Prediction", "👨‍💻 Creator Info"])
# 
# # Load model once (used in prediction page)
# @st.cache_resource
# def load_model():
#     return mlflow.pyfunc.load_model("runs:/3fae881bd193481c83962b51304f937c/model")
# 
# model = load_model()
# 
# # Page 1: Project Introduction
# if page == " Project Introduction":
#     st.title("SmartPremium: Insurance Cost Prediction App")
#     st.markdown("""
#         This application predicts the **insurance premium** based on user inputs like:
#         - Age, gender, income, vehicle details
#         - Health score, previous claims, habits
#         - Policy duration, location, and more.
# 
#         ###  Behind the scenes:
#         - Trained using advanced machine learning models like **XGBoost**
#         - Feature engineered and tuned for best performance
#         - Model logged & served using **MLflow**
# 
#         ---
# 
#     """)
# 
# # Page 2: Premium Prediction
# elif page == " Premium Prediction":
#     st.title("Insurance Premium Predictor")
# 
#     # Input form
#     id = st.number_input("ID", value=1)
#     age = st.slider("Age", 18, 80, 30)
#     gender = st.selectbox("Gender", ["Male", "Female"])
#     annual_income = st.number_input("Annual Income", value=500000.0)
#     marital_status = st.selectbox("Marital Status", ["Single", "Married"])
#     dependents = st.number_input("Number of Dependents", value=0)
#     education = st.selectbox("Education Level", ["Graduate", "High School"])
#     occupation = st.selectbox("Occupation", ["Salaried", "Self-Employed"])
#     health_score = st.slider("Health Score", 0, 100, 75)
#     location = st.selectbox("Location", ["North", "South", "East", "West"])
#     policy_type = st.selectbox("Policy Type", ["Basic", "Comprehensive"])
#     previous_claims = st.number_input("Previous Claims", value=0)
#     vehicle_age = st.number_input("Vehicle Age (years)", value=2)
#     credit_score = st.number_input("Credit Score", value=700)
#     insurance_duration = st.number_input("Insurance Duration (years)", value=5)
#     policy_start_date = st.text_input("Policy Start Date (YYYY-MM-DD)", value="2020-01-01")
#     feedback = st.selectbox("Customer Feedback", ["Good", "Average", "Poor"])
#     smoking = st.selectbox("Smoking Status", ["yes", "no"])
#     exercise = st.selectbox("Exercise Frequency", ["Daily", "Occasionally", "Never"])
#     property_type = st.selectbox("Property Type", ["House", "Apartment"])
# 
#     # Create input DataFrame
#     input_df = pd.DataFrame([{
#         "id": id,
#         "Age": age,
#         "Gender": gender,
#         "Annual Income": annual_income,
#         "Marital Status": marital_status,
#         "Number of Dependents": dependents,
#         "Education Level": education,
#         "Occupation": occupation,
#         "Health Score": health_score,
#         "Location": location,
#         "Policy Type": policy_type,
#         "Previous Claims": previous_claims,
#         "Vehicle Age": vehicle_age,
#         "Credit Score": credit_score,
#         "Insurance Duration": insurance_duration,
#         "Policy Start Date": policy_start_date,
#         "Customer Feedback": feedback,
#         "Smoking Status": smoking,
#         "Exercise Frequency": exercise,
#         "Property Type": property_type
#     }])
# 
#     # Convert types
#     input_df["id"] = input_df["id"].astype(int)
#     input_df["Age"] = input_df["Age"].astype(float)
#     input_df["Annual Income"] = input_df["Annual Income"].astype(float)
#     input_df["Number of Dependents"] = input_df["Number of Dependents"].astype(float)
#     input_df["Health Score"] = input_df["Health Score"].astype(float)
#     input_df["Previous Claims"] = input_df["Previous Claims"].astype(float)
#     input_df["Vehicle Age"] = input_df["Vehicle Age"].astype(float)
#     input_df["Credit Score"] = input_df["Credit Score"].astype(float)
#     input_df["Insurance Duration"] = input_df["Insurance Duration"].astype(float)
# 
#     if st.button("Predict"):
#         try:
#             prediction = model.predict(input_df)
#             st.success(f" Predicted Premium: ₹{prediction[0]:,.2f}")
#         except Exception as e:
#             st.error(f" Prediction failed: {e}")
# 
# # Page 3: Creator Info
# elif page == " Creator Info":
#     st.title(" Creator Information")
#     st.markdown("""
#     **Name:** Amogh Chetty
# 
#     **Project Title:** SmartPremium: Predicting Insurance Costs with Machine Learning
# 
#     **Technologies Used:** Python, Streamlit, MLflow, XGBoost, Scikit-learn, Pandas
# 
#     **Deployment:** Streamlit app using MLflow-tracked model
# 
#     **Inspiration:** Real-world use case of automating premium estimation to reduce manual bias and improve customer transparency.
# 
#     ---
# 
#     """)
# 
#

from pyngrok import ngrok
import threading
import time
import os

# Set your NGROK auth token (replace with your actual token)
ngrok.set_auth_token("30KwR5ga0Vn9OtYNtI04zeZnnVL_6MiSjKv2387sQbijjpHSS")

# Function to run streamlit
def run_streamlit():
    os.system("streamlit run app.py")

# Start streamlit in a new thread
thread = threading.Thread(target=run_streamlit)
thread.start()

# Wait a few seconds for the app to start
time.sleep(5)

# Expose port 8501
public_url = ngrok.connect(8501)
print(f"🚀 Streamlit app is live at: {public_url}")

